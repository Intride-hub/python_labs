def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    res=text
    for char in ['\n','\r','\t']:
        res=res.replace(char,' ')
    
    if yo2e:
        res=res.replace('—ë','–µ').replace('–Å','–ï')
    
    if casefold:
        res=res.casefold()
    res=' '.join(res.split())


    return res

print(repr(normalize("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t")))


print(repr(normalize("—ë–∂–∏–∫, –Å–ª–∫–∞")))


print(repr(normalize("Hello\r\nWorld")))


print(repr(normalize("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  ")))



import re

def tokenize(text: str) -> list[str]:
    token = re.findall(r"[\w]+(?:-[\w]+)*", text)
    return token

print(tokenize("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"))

 
print(tokenize("hello,world!!!"))

print(tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ"))

print(tokenize("2025 –≥–æ–¥"))

print(tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"))


def count_freq(tokens: list[str]) -> dict[str, int]:
    freq={}
    for token in tokens:
        if token in freq:
            freq[token]+=1
        else:
            freq[token]=1
    return freq

print(count_freq(["a","b","a","c","b","a"]))

print(count_freq(["bb","aa","bb","aa","cc"]))

def top_n(freq: dict[str, int], n: int = 5) -> list[tuple[str, int]]:
    sort_it=sorted(freq.items(), key=lambda x: (-x[1],x[0]))
    return sort_it[:n]


print(top_n(count_freq(["a","b","a","c","b","a"]),2))
print(top_n(count_freq(["bb","aa","bb","aa","cc"]),2))







print("–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤...")
    
        # –¢–µ—Å—Ç—ã normalize
assert normalize("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t") == "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"
assert normalize("—ë–∂–∏–∫, –Å–ª–∫–∞") == "–µ–∂–∏–∫, –µ–ª–∫–∞"
assert normalize("Hello\r\nWorld") == "hello world"
assert normalize("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  ") == "–¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã"
print("‚úì normalize —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã")
    
    # –¢–µ—Å—Ç—ã tokenize
assert tokenize("–ø—Ä–∏–≤–µ—Ç, –º–∏—Ä!") == ["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä"]
assert tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ") == ["–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É", "–∫—Ä—É—Ç–æ"]
assert tokenize("2025 –≥–æ–¥") == ["2025", "–≥–æ–¥"]
assert tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ") == ["emoji", "–Ω–µ", "—Å–ª–æ–≤–æ"]
print("‚úì tokenize —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã")
    
    # –¢–µ—Å—Ç—ã count_freq + top_n
freq = count_freq(["a", "b", "a", "c", "b", "a"])
assert freq == {"a": 3, "b": 2, "c": 1}
assert top_n(freq, 2) == [("a", 3), ("b", 2)]
    
    # –¢–∞–π-–±—Ä–µ–π–∫ –ø–æ —Å–ª–æ–≤—É –ø—Ä–∏ —Ä–∞–≤–Ω–æ–π —á–∞—Å—Ç–æ—Ç–µ
freq2 = count_freq(["bb", "aa", "bb", "aa", "cc"])
assert top_n(freq2, 2) == [("aa", 2), ("bb", 2)]
print("‚úì count_freq + top_n —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã")

print("–í—Å–µ —Ç–µ—Å—Ç—ã —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–π–¥–µ–Ω—ã")


