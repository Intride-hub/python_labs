–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ 1


–ó–∞–¥–∞–Ω–∏–µ 1


<p>name=input('–ò–º—è:')<br>
age=int(input('–í–æ–∑—Ä–∞—Å—Ç:'))<br>
print(f'–ü—Ä–∏–≤–µ—Ç, {name}! –ß–µ—Ä–µ–∑ –≥–æ–¥ —Ç–µ–±–µ –±—É–¥–µ—Ç {age+1}.')</p>

![–ó–∞–¥–∞–Ω–∏–µ 1](misc/img/lab01/lab01_01.png)


–ó–∞–¥–∞–Ω–∏–µ 2

<p>a=float(input('a: ').replace(',','.'))<br>
b=float(input('b: ').replace(',','.'))<br>
sum=a+b<br>
avg=(a+b)/2<br>
print(f'sum={sum:.2f}; avg={avg:.2f}')</p>



![–ó–∞–¥–∞–Ω–∏–µ 1](misc/img/lab01/lab01_02.png)


–ó–∞–¥–∞–Ω–∏–µ 3


<p>price = float(input("price: "))<br>
discount = float(input("discount: "))<br>
vat = float(input("vat: "))<br>
base= price * (1-discount/100)<br>
vat_amount= base * (vat/100)<br>
total = base + vat_amount<br>
print(f'–ë–∞–∑–∞ –ø–æ—Å–ª–µ —Å–∫–∏–¥–∫–∏: {base:.2f} ‚ÇΩ')<br>
print(f'–ù–î–°: {vat_amount:.2f} ‚ÇΩ')<br>
print(f'–ò—Ç–æ–≥–æ –∫ –æ–ø–ª–∞—Ç–µ: {total:.2f} ‚ÇΩ')</p>

![–ó–∞–¥–∞–Ω–∏–µ 1](misc/img/lab01/lab01_03.png)


–ó–∞–¥–∞–Ω–∏–µ 4


<p>m=int(input("–ú–∏–Ω—É—Ç—ã: "))<br>
print(f'{m//60}:{m%60}')</p>

![–ó–∞–¥–∞–Ω–∏–µ 1](misc/img/lab01/lab01_04.png)


–ó–∞–¥–∞–Ω–∏–µ 5


<p>name=input('–§–ò–û: ')<br>
namecl=name.strip()<br>
words = namecl.split()<br>
init=[]<br>
for w in words:<br>
    fir = w[0]<br>
    up = fir.upper()<br>
    init.append(up)<br>
initre=''.join(init)<br>
lenth=sum(len(word) for word in words) + (len(words)-1)<br>
print(f'–ò–Ω–∏—Ü–∏–∞–ª—ã: {initre}')<br>
print(f'–î–ª–∏–Ω–∞ (—Å–∏–º–≤–æ–ª—ã): {lenth}')</p>


![–ó–∞–¥–∞–Ω–∏–µ 1](misc/img/lab01/lab01_05.png)



–∑–∞–¥–∞–Ω–∏–µ 6

```python
<p>n=int(input('in_1: '))<br>
och=0<br>
zaoch=0<br>
for i in range(n):<br>
    line = input(f'in_{i+2}: ').split()<br>
    format=line[-1]<br>
    if format == "True":<br>
        och+=1<br>
    else:<br>
        zaoch+=1<br>
print(f'out: {och} {zaoch}')</p>
```


![–ó–∞–¥–∞–Ω–∏–µ 1](misc/img/lab01/lab01_06.png)



–ó–∞–¥–∞–Ω–∏–µ 7

<p>s=input('in: ')<br>
def stroka(s):<br>
    start = next(i for i, c in enumerate(s) if c.isupper())<br>
    step = (next(i for i, c in enumerate(s) if c.isdigit()) + 1) - start<br>
    result = ""<br>
    for i in range(start, len(s), step):<br>
        result += s[i]<br>
        if s[i] == ".":<br>
            break<br>
    return result<br>


print(f'out: {stroka(s)}')</p>


![–ó–∞–¥–∞–Ω–∏–µ 7](misc/img/lab01/lab01_07.png)



–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ 2

–ó–∞–¥–∞–Ω–∏–µ 1

```python
def min_max(nums: list[float | int]) -> tuple[float | int, float | int]:
    if not nums:
        raise ValueError
    return (min(nums),max(nums))



def unique_sorted(nums: list[float | int]) -> list[float | int]:
    a=set(nums)
    return sorted(a)


def flatten(mat: list[list | tuple]) -> list:
    res=[]
    for row in mat:
        if not isinstance(row, (list,tuple)):
           raise TypeError
        res.extend(row)     
    return res

print(min_max([3, -1, 5, 5, 0]))
print(min_max([42]))
print(min_max([-5, -2, -9]))
print(min_max([1.5, 2, 2.0, -3.1]))
print(min_max([]))

print(unique_sorted([3, 1, 2, 1, 3]))
print(unique_sorted([-1, -1, 0, 2, 2]))
print(unique_sorted([1.0, 1, 2.5, 2.5, 0]))
print(unique_sorted([]))


print(flatten([[1, 2], [3, 4]]))
print(flatten([[1, 2], (3, 4, 5)]))
print(flatten([[1], [], [2, 3]]))
print(flatten([[1, 2], "ab"]))
```
![–ó–∞–¥–∞–Ω–∏–µ 1](misc/img/lab02/lab02_arrays1.png)
![–ó–∞–¥–∞–Ω–∏–µ A](misc/img/lab02/lab02_arrays2.png)
![–ó–∞–¥–∞–Ω–∏–µ B](misc/img/lab02/lab02_arrays3.png)




–ó–∞–¥–∞–Ω–∏–µ B


```python
def check(mat: list[list])-> None:
    if not mat:
        return
    ln=len(mat[0])
    for i in mat:
        if len(i)!=ln:
            raise ValueError




def transpose(mat: list[list[float | int]]) -> list[list]:
    check(mat)
    if not mat:
        return []
    return[list(col) for col in zip(*mat)]


def row_sums(mat: list[list[float | int]]) -> list[float]:
    check(mat)
    return[sum(row) for row in mat]

def col_sums(mat: list[list[float | int]]) -> list[float]:
    check(mat)
    if not mat:
        return []
    return[sum(col) for col in zip(*mat)]
    
print(transpose([[1, 2, 3]]))     
print(transpose([[1], [2], [3]]))  
print(transpose([[1, 2], [3, 4]]))   
print(transpose([]))                
print(transpose([[1, 2], [3]]))          

print(row_sums([[1, 2, 3], [4, 5, 6]]))  
print(row_sums([[-1, 1], [10, -10]]))   
print(row_sums([[0, 0], [0, 0]]))    
print(row_sums([[1,2],[3]]))    

print(col_sums([[1, 2, 3], [4, 5, 6]]))  
print(col_sums([[-1, 1], [10, -10]]))  
print(col_sums([[0, 0], [0, 0]]))       
print(col_sums([[1, 2], [3]]))
```

![–ó–∞–¥–∞–Ω–∏–µ B](misc/img/lab02/lab02_matrix.png)
![–ó–∞–¥–∞–Ω–∏–µ B](misc/img/lab02/lab02_matrix_2.png)
![–ó–∞–¥–∞–Ω–∏–µ B](misc/img/lab02/lab02_matrix_3.png)



–ó–∞–¥–∞–Ω–∏–µ C

```python
def format_record(rec: tuple[str, str, float]) -> str:
    if not isinstance(rec, tuple) or len(rec) != 3:
        raise ValueError("–û–∂–∏–¥–∞–µ—Ç—Å—è –∫–æ—Ä—Ç–µ–∂ –∏–∑ —Ç—Ä—ë—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (fio, group, gpa)")

    fio, group, gpa = rec

    if not isinstance(fio, str):
        raise TypeError("–§–ò–û –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π")
    
    if not isinstance(group, str):
        raise TypeError("–ì—Ä—É–ø–ø–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π")
    
    if not isinstance(gpa, (int, float)):
        raise TypeError("GPA –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —á–∏—Å–ª–æ–º")
    
    fio_clean=" ".join(fio.strip().split())

    parts=fio_clean.split()

    if len(parts)<2 or len(parts)>3:
        raise ValueError(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –§–ò–û: '{fio}'")
    
    surname = parts[0].capitalize()

    initials= "".join(p[0].upper() + '.' for p in parts[1:])

    group_clean=group.strip()

    if not group_clean:
        raise ValueError("–ì—Ä—É–ø–ø–∞ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç–æ–π")
    
    gpa_str=f"{float(gpa):.2f}"

    return f"{surname} {initials}, –≥—Ä. {group_clean}, GPA {gpa_str}"

print(format_record(("–ò–≤–∞–Ω–æ–≤ –ò–≤–∞–Ω –ò–≤–∞–Ω–æ–≤–∏—á", "BIVT-25", 4.6)))

print(format_record(("–ü–µ—Ç—Ä–æ–≤ –ü—ë—Ç—Ä", "IKBO-12", 5.0)))

print(format_record(("–ü–µ—Ç—Ä–æ–≤ –ü—ë—Ç—Ä –ü–µ—Ç—Ä–æ–≤–∏—á", "IKBO-12", 5.0)))

print(format_record(("  —Å–∏–¥–æ—Ä–æ–≤–∞  –∞–Ω–Ω–∞   —Å–µ—Ä–≥–µ–µ–≤–Ω–∞ ", "ABB-01", 3.999)))
```
![–ó–∞–¥–∞–Ω–∏–µ C](misc/img/lab02/lab02_tuples.png)




–õ–∞–±–∞—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ 3


–ó–∞–¥–∞–Ω–∏–µ A
normalize
![–ó–∞–¥–∞–Ω–∏–µ B](misc/img/lab03/lab03_1.png)
```python
def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    res=text
    for char in ['\n','\r','\t']:
        res=res.replace(char,' ')
    
    if yo2e:
        res=res.replace('—ë','–µ').replace('–Å','–ï')
    
    if casefold:
        res=res.casefold()
    res=' '.join(res.split())


    return res

print(repr(normalize("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t")))


print(repr(normalize("—ë–∂–∏–∫, –Å–ª–∫–∞")))


print(repr(normalize("Hello\r\nWorld")))


print(repr(normalize("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  ")))
```

tokenize


![–ó–∞–¥–∞–Ω–∏–µ C](misc/img/lab03/lab03_1_2.png)



```python
import re

def tokenize(text: str) -> list[str]:
    token = re.findall(r"[\w]+(?:-[\w]+)*", text)
    return token

print(tokenize("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"))

 
print(tokenize("hello,world!!!"))

print(tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ"))

print(tokenize("2025 –≥–æ–¥"))

print(tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"))
```




count_freq 
![–ó–∞–¥–∞–Ω–∏–µ C](misc/img/lab03/lab03_1_3.png)

```python
def count_freq(tokens: list[str]) -> dict[str, int]:
    freq={}
    for token in tokens:
        if token in freq:
            freq[token]+=1
        else:
            freq[token]=1
    return freq

print(count_freq(["a","b","a","c","b","a"]))

print(count_freq(["bb","aa","bb","aa","cc"]))


```

top_n


![–ó–∞–¥–∞–Ω–∏–µ C](misc/img/lab03/lab03_1_4.png)


```python
def top_n(freq: dict[str, int], n: int = 5) -> list[tuple[str, int]]:
    sort_it=sorted(freq.items(), key=lambda x: (-x[1],x[0]))
    return sort_it[:n]


print(top_n(count_freq(["a","b","a","c","b","a"]),2))
print(top_n(count_freq(["bb","aa","bb","aa","cc"]),2))

```


Mini test

![–ó–∞–¥–∞–Ω–∏–µ C](misc/img/lab03/lab03_mini_test.png)

```python
print("–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤...")
    
        # –¢–µ—Å—Ç—ã normalize
assert normalize("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t") == "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"
assert normalize("—ë–∂–∏–∫, –Å–ª–∫–∞") == "–µ–∂–∏–∫, –µ–ª–∫–∞"
assert normalize("Hello\r\nWorld") == "hello world"
assert normalize("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  ") == "–¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã"
print("‚úì normalize —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã")
    
    # –¢–µ—Å—Ç—ã tokenize
assert tokenize("–ø—Ä–∏–≤–µ—Ç, –º–∏—Ä!") == ["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä"]
assert tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ") == ["–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É", "–∫—Ä—É—Ç–æ"]
assert tokenize("2025 –≥–æ–¥") == ["2025", "–≥–æ–¥"]
assert tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ") == ["emoji", "–Ω–µ", "—Å–ª–æ–≤–æ"]
print("‚úì tokenize —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã")
    
    # –¢–µ—Å—Ç—ã count_freq + top_n
freq = count_freq(["a", "b", "a", "c", "b", "a"])
assert freq == {"a": 3, "b": 2, "c": 1}
assert top_n(freq, 2) == [("a", 3), ("b", 2)]
    
    # –¢–∞–π-–±—Ä–µ–π–∫ –ø–æ —Å–ª–æ–≤—É –ø—Ä–∏ —Ä–∞–≤–Ω–æ–π —á–∞—Å—Ç–æ—Ç–µ
freq2 = count_freq(["bb", "aa", "bb", "aa", "cc"])
assert top_n(freq2, 2) == [("aa", 2), ("bb", 2)]
print("‚úì count_freq + top_n —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã")

print("–í—Å–µ —Ç–µ—Å—Ç—ã —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–π–¥–µ–Ω—ã")

```


–ó–∞–¥–∞–Ω–∏–µ B


![–ó–∞–¥–∞–Ω–∏–µ C](misc/img/lab03/lab03_text_1.png)

```python
import sys
from  text import *
from pathlib import Path

sys.path.append(str(Path(__file__).resolve().parents[1]))

def main():
    text =sys.stdin.read() 

    if not text:
        print("–í–≤–æ–¥ –ø—É—Å—Ç")
        return
    
    
    normalized_text = normalize(text)
    tokens = tokenize(normalized_text)
    freq_dict = count_freq(tokens)
    
    total_words = len(tokens)
    unique_words = len(freq_dict)
    
   
    print(f"–í—Å–µ–≥–æ —Å–ª–æ–≤: {total_words}")
    print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {unique_words}")
    
 
    print("–¢–æ–ø-5:")
    top_words = top_n(freq_dict, 5)
    for word, count in top_words:
        print(f"{word}:{count}")
 
if __name__ == "__main__":
    main()
```





<h1>–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è –†–∞–±–æ—Ç–∞ 4</h1>

<h2>–ó–∞–¥–∞–Ω–∏–µ A</h2>

```python
from pathlib import Path
import csv


def read_text(path: str | Path, encoding: str = "utf-8") -> str:
    p=Path(path)
    return p.read_text(encoding=encoding)

def write_csv(rows: list[tuple | list], path: str | Path, header: tuple[str, ...] | None = None) -> None:
    p = Path(path)
    rows_list = list(rows)

    if rows_list:
        first_len = len(rows_list[0])
        for i, row in enumerate(rows_list):
            if len(row) != first_len:
                raise ValueError(f"–°—Ç—Ä–æ–∫–∞ {i} –∏–º–µ–µ—Ç –¥–ª–∏–Ω—É {len(row)}, –æ–∂–∏–¥–∞–µ—Ç—Å—è {first_len}")
            
    with open(path, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        if header is not None:
            writer.writerow(header)
        writer.writerows(rows_list)
```


<h2>Mini test</h2>


![–ó–∞–¥–∞–Ω–∏–µ C](misc/img/lab04/mini_test.png)


<h2>–ó–∞–¥–∞–Ω–∏–µ B</h2>


```python
import sys
import argparse
from pathlib import Path
from collections import Counter

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –∏–∑ lib
sys.path.insert(0, str(Path(__file__).parent.parent))

from io_txt_csv import read_text, write_csv


def frequencies_from_text(text: str) -> dict[str, int]:
    from lib.text import normalize, tokenize
    
    tokens = tokenize(normalize(text))
    return Counter(tokens)


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∫—Ä–∏–ø—Ç–∞."""
    parser = argparse.ArgumentParser(description='–ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ')
    parser.add_argument('--input', '-i', default='src/data/lab04/input.txt',
                       help='–í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: src/data/lab04/input.txt)')
    parser.add_argument('--output', '-o', default='src/data/lab04/report.csv',
                       help='–í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: src/data/lab04/report.csv)')
    parser.add_argument('--encoding', '-e', default='utf-8',
                       help='–ö–æ–¥–∏—Ä–æ–≤–∫–∞ —Ñ–∞–π–ª–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: utf-8)')
    
    args = parser.parse_args()
    
    try:
        # –ß—Ç–µ–Ω–∏–µ –∏ –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å —É–∫–∞–∑–∞–Ω–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
        text = read_text(args.input, encoding=args.encoding)
        freq = frequencies_from_text(text)
        from lib.text import top_n
        sorted_freq = top_n(freq, len(freq))  # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å–ª–æ–≤–∞ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏
        
        # –ü–æ–¥—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        total_words = sum(freq.values())
        unique_words = len(freq)
        
        # –ó–∞–ø–∏—Å—å –≤ CSV
        write_csv(sorted_freq, args.output, header=("word", "count"))
        
        # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤ –∫–æ–Ω—Å–æ–ª—å
        print(f"–í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª: {args.input}")
        print(f"–ö–æ–¥–∏—Ä–æ–≤–∫–∞: {args.encoding}")
        print(f"–í—Å–µ–≥–æ —Å–ª–æ–≤: {total_words}")
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {unique_words}")
        print("–¢–æ–ø-5:")
        
        # –í—ã–≤–æ–¥ —Ç–æ–ø-5 —Å–ª–æ–≤
        top_5 = sorted_freq[:5]
        for word, count in top_5:
            print(f"{word}:{count}")
            
        print(f"\n–û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {args.output}")
        
    except FileNotFoundError:
        print(f"–û—à–∏–±–∫–∞: –§–∞–π–ª '{args.input}' –Ω–µ –Ω–∞–π–¥–µ–Ω")
        print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
        sys.exit(1)
    except UnicodeDecodeError as e:
        print(f"–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        print(f"–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–∫–∞–∑–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –∫–æ–¥–∏—Ä–æ–≤–∫—É: --encoding cp1251")
        sys.exit(1)


if __name__ == "__main__":
    main()
```

<h1>–¢–µ—Å—Ç –∫–µ–π—Å</h1>
<h2>A</h2>

![–ó–∞–¥–∞–Ω–∏–µ C](misc/img/lab04/text_report_A.png)

<h2>B</h2>

![–ó–∞–¥–∞–Ω–∏–µ C](misc/img/lab04/text_report_B.png)


<h2>C</h2>

![–ó–∞–¥–∞–Ω–∏–µ C](misc/img/lab04/text_report_C.png)
